{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import pandas as pd\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(1) # set a seed so that the results are consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset ##\n",
    "\n",
    "First, let's get the dataset you will work on. The following code will load protein data set in train variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['GC_case, case=1, control=0', '781010_HP1118', '780792_hslU',\n",
      "       '781329_rplL', '780914_HP0243', '781118_HP1293', '781673_HP0385',\n",
      "       '780753_tig', '780459_recA', '780849_HP0875', '781632_HP0371',\n",
      "       '780394_groES', '780913_HP0701', '781516_HP1172', '781139_tsf',\n",
      "       '781205_HP0779', '781538_fabG', '780927_HP1379', '780670_acpP',\n",
      "       '780985_HP0480', '781593_HP0073'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GC_case, case=1, control=0</th>\n",
       "      <th>781010_HP1118</th>\n",
       "      <th>780792_hslU</th>\n",
       "      <th>781329_rplL</th>\n",
       "      <th>780914_HP0243</th>\n",
       "      <th>781118_HP1293</th>\n",
       "      <th>781673_HP0385</th>\n",
       "      <th>780753_tig</th>\n",
       "      <th>780459_recA</th>\n",
       "      <th>780849_HP0875</th>\n",
       "      <th>...</th>\n",
       "      <th>780394_groES</th>\n",
       "      <th>780913_HP0701</th>\n",
       "      <th>781516_HP1172</th>\n",
       "      <th>781139_tsf</th>\n",
       "      <th>781205_HP0779</th>\n",
       "      <th>781538_fabG</th>\n",
       "      <th>780927_HP1379</th>\n",
       "      <th>780670_acpP</th>\n",
       "      <th>780985_HP0480</th>\n",
       "      <th>781593_HP0073</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.479</td>\n",
       "      <td>2.417</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.114</td>\n",
       "      <td>2.274</td>\n",
       "      <td>...</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1.868</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.465</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.424</td>\n",
       "      <td>1.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.821</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.015</td>\n",
       "      <td>1.294</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.079</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.021</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.525</td>\n",
       "      <td>1.457</td>\n",
       "      <td>1.317</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.713</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.495</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.463</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1.451</td>\n",
       "      <td>1.139</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1.542</td>\n",
       "      <td>2.207</td>\n",
       "      <td>0.232</td>\n",
       "      <td>2.371</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0.881</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.015</td>\n",
       "      <td>2.231</td>\n",
       "      <td>2.213</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.525</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.156</td>\n",
       "      <td>1.402</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.108</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.326</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1.830</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.020</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.241</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GC_case, case=1, control=0  781010_HP1118  780792_hslU  781329_rplL  \\\n",
       "0                           0          0.533        0.000        1.479   \n",
       "1                           0          1.868        0.000        2.465   \n",
       "2                           0          0.000        0.008        0.091   \n",
       "3                           0          0.318        0.000        1.079   \n",
       "4                           0          0.918        0.016        1.525   \n",
       "5                           0          0.021        0.007        0.530   \n",
       "6                           0          0.000        0.018        0.520   \n",
       "7                           0          1.542        2.207        0.232   \n",
       "8                           0          0.881        0.004        1.015   \n",
       "9                           0          1.830        0.000        0.050   \n",
       "\n",
       "   780914_HP0243  781118_HP1293  781673_HP0385  780753_tig  780459_recA  \\\n",
       "0          2.417          0.913          0.019       0.658        0.114   \n",
       "1          0.020          2.424          1.515       0.000        1.821   \n",
       "2          0.000          0.028          0.000       0.000        0.020   \n",
       "3          0.004          0.107          0.026       0.632        0.184   \n",
       "4          1.457          1.317          0.000       0.020        0.140   \n",
       "5          0.014          1.463          0.023       0.032        0.015   \n",
       "6          0.000          0.000          0.010       0.005        0.020   \n",
       "7          2.371          0.049          0.061       0.000        0.040   \n",
       "8          2.231          2.213          0.000       0.000        1.525   \n",
       "9          0.002          0.012          0.008       0.124        0.239   \n",
       "\n",
       "   780849_HP0875      ...        780394_groES  780913_HP0701  781516_HP1172  \\\n",
       "0          2.274      ...               1.968          0.007          0.000   \n",
       "1          0.001      ...               0.000          0.017          0.031   \n",
       "2          0.020      ...               0.015          0.000          0.000   \n",
       "3          0.021      ...               0.027          0.055          0.035   \n",
       "4          0.145      ...               0.713          1.347          0.034   \n",
       "5          0.017      ...               0.001          0.036          0.057   \n",
       "6          0.000      ...               0.035          0.000          0.000   \n",
       "7          0.017      ...               0.000          0.042          0.013   \n",
       "8          0.000      ...               1.156          1.402          0.029   \n",
       "9          0.020      ...               0.000          0.000          0.241   \n",
       "\n",
       "   781139_tsf  781205_HP0779  781538_fabG  780927_HP1379  780670_acpP  \\\n",
       "0       0.022          0.000        0.030          0.027        0.000   \n",
       "1       0.000          0.025        0.015          1.294        0.000   \n",
       "2       0.000          0.022        0.000          0.000        0.114   \n",
       "3       0.968          0.019        0.795          0.076        0.043   \n",
       "4       0.500          0.187        1.495          0.847        0.396   \n",
       "5       0.022          1.451        1.139          0.103        0.025   \n",
       "6       0.000          0.010        0.000          0.000        0.009   \n",
       "7       0.019          0.004        0.043          0.027        0.000   \n",
       "8       0.000          0.108        0.174          0.007        0.326   \n",
       "9       0.000          0.000        0.002          0.002        0.011   \n",
       "\n",
       "   780985_HP0480  781593_HP0073  \n",
       "0          0.018          0.290  \n",
       "1          0.057          0.000  \n",
       "2          0.000          0.031  \n",
       "3          0.273          0.009  \n",
       "4          0.404          0.034  \n",
       "5          0.074          0.055  \n",
       "6          0.000          0.000  \n",
       "7          0.030          0.000  \n",
       "8          0.016          0.860  \n",
       "9          0.000          0.034  \n",
       "\n",
       "[10 rows x 21 columns]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train  = pd.read_csv('proteinData.csv')\n",
    "columns = train.columns\n",
    "print (columns)\n",
    "train.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test seperation ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.881,  0.004,  1.015,  2.231,  2.213,  0.   ,  0.   ,  1.525,\n",
       "        0.   ,  2.325,  1.156,  1.402,  0.029,  0.   ,  0.108,  0.174,\n",
       "        0.007,  0.326,  0.016,  0.86 ])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y = train[\"GC_case, case=1, control=0\"][8:288]\n",
    "train_X = train.drop([\"GC_case, case=1, control=0\"],axis=1)[8:288]\n",
    "test_X = train.drop([\"GC_case, case=1, control=0\"],axis=1)[:8].append(train.drop([\"GC_case, case=1, control=0\"],axis=1)[288:]) \n",
    "test_Y = train[\"GC_case, case=1, control=0\"][0:8].append(train[\"GC_case, case=1, control=0\"][288:])\n",
    "\n",
    "train_X.shape, train_Y.shape,  test_X.shape, test_Y.shape\n",
    "train_Y = train_Y.values\n",
    "train_X = train_X.values\n",
    "test_X = test_X.values\n",
    "test_Y = test_Y.values\n",
    "train_Y = train_Y.reshape((train_Y.shape[0], 1))\n",
    "test_Y = test_Y.reshape((test_Y.shape[0], 1))\n",
    "# train_X, train_Y, test_X, test_Y\n",
    "train_Y.shape, test_Y.shape\n",
    "train_X[0].T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape of dataset ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X is: (280, 20)\n",
      "The shape of Y is: (280, 1)\n",
      "I have m = 280 training examples!\n"
     ]
    }
   ],
   "source": [
    "shape_X = train_X.shape\n",
    "shape_Y = train_Y.shape\n",
    "m =  train_Y.size # training set size\n",
    "\n",
    "\n",
    "print ('The shape of X is: ' + str(shape_X))\n",
    "print ('The shape of Y is: ' + str(shape_Y))\n",
    "print ('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Logistic Regression\n",
    "\n",
    "Before building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn's built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.285714285714292"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the logistic regression classifier\n",
    "\n",
    "clf = sklearn.linear_model.LogisticRegressionCV();\n",
    "clf.fit(train_X , train_Y.ravel());\n",
    "predictions = clf.predict(test_X)\n",
    "score = clf.score(test_X,test_Y)\n",
    "score * 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network model\n",
    "\n",
    "Logistic regression did not work well on the \"Protein dataset\". Lets train to train a Neural Network with a single hidden layer.\n",
    "\n",
    "\n",
    "**Mathematically**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}\\tag{1}$$ \n",
    "$$a^{[1] (i)} = \\tanh(z^{[1] (i)})\\tag{2}$$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\\tag{3}$$\n",
    "$$\\hat{y}^{(i)} = a^{[2] (i)} = \\sigma(z^{ [2] (i)})\\tag{4}$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\mbox{if } a^{[2](i)} > 0.5 \\\\ 0 & \\mbox{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n",
    "Given the predictions on all the examples, you can also compute the cost $J$ as follows: \n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large\\left(\\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right)  \\large  \\right) \\small \\tag{6}$$\n",
    "\n",
    "**Reminder**: The general methodology to build a Neural Network is to:\n",
    "    1. Define the neural network structure ( # of input units,  # of hidden units, etc). \n",
    "    2. Initialize the model's parameters\n",
    "    3. Loop:\n",
    "        - Implement forward propagation\n",
    "        - Compute loss\n",
    "        - Implement backward propagation to get the gradients\n",
    "        - Update parameters (gradient descent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the neural network structure ####\n",
    "\n",
    "**Exercise**: Define three variables:\n",
    "    - n_x: the size of the input layer\n",
    "    - n_h: the size of the hidden layer (set this to 4) \n",
    "    - n_y: the size of the output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    \n",
    "    n_x = X.shape[0] # size of input layer\n",
    "    n_h = 4\n",
    "    n_y = Y.shape[0] # size of output layer\n",
    "   \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Function ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1. / ( 1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model's parameters ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random.\n",
    "    \n",
    "   \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "   \n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propogation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "   \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    #print(W1.shape, b1.shape, W2.shape, b2.shape)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    \n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Cost ###\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 0}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small\\tag{13}$$\n",
    "\n",
    "\n",
    "- There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented\n",
    "$- \\sum\\limits_{i=0}^{m}  y^{(i)}\\log(a^{[2](i)})$:\n",
    "```python\n",
    "logprobs = np.multiply(np.log(A2),Y)\n",
    "cost = - np.sum(logprobs)                \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y, parameters):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of example\n",
    "    \n",
    "    # Retrieve W1 and W2 from parameters\n",
    "  \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    \n",
    "    # Compute the cross-entropy cost\n",
    "   \n",
    "    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1-Y), np.log(1-A2))\n",
    "    \n",
    "    cost = -1/m * np.sum(logprobs) \n",
    "    \n",
    "    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n",
    "                                # E.g., turns [[17]] into 17 \n",
    "    assert(isinstance(cost, float))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propogation ###\n",
    "Using the cache computed during forward propagation, you can now implement backward propagation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "  \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "  \n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "   \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "   \n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    \n",
    "    dZ2= A2 - Y\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), (1 - np.power(A1, 2)))\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "   \n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update Parameters ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.5):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    \n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate parts all parts in nn_model() ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "       \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Predictions\n",
    "\n",
    "predictions = $y_{prediction} = \\mathbb 1 \\text{{activation > 0.5}} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "   \n",
    "    A2, cache = forward_propagation(X, parameters)\n",
    "    predictions = (A2 > 0.5)  # Vectorized\n",
    "   \n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Prediction ###\n",
    "It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of $n_h$ hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693113\n",
      "Cost after iteration 1000: 0.588662\n",
      "Cost after iteration 2000: 0.263845\n",
      "Cost after iteration 3000: 0.214621\n",
      "Cost after iteration 4000: 0.182412\n",
      "Cost after iteration 5000: 0.168353\n",
      "Cost after iteration 6000: 0.155555\n",
      "Cost after iteration 7000: 0.142367\n",
      "Cost after iteration 8000: 0.130666\n",
      "Cost after iteration 9000: 0.122882\n"
     ]
    }
   ],
   "source": [
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = nn_model(train_X.T, train_Y.T, n_h = 15, num_iterations = 10000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93%\n"
     ]
    }
   ],
   "source": [
    "# Print accuracy\n",
    "predictions = predict(parameters, train_X.T)\n",
    "print ('Accuracy: %d' % float((np.dot(train_Y.T,predictions.T) + np.dot(1-train_Y.T,1-predictions.T))/float(train_Y.T.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Data Prediction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693167\n",
      "Cost after iteration 1000: 0.023331\n",
      "Cost after iteration 2000: 0.006017\n",
      "Cost after iteration 3000: 0.003081\n",
      "Cost after iteration 4000: 0.001993\n",
      "Cost after iteration 5000: 0.001446\n",
      "Cost after iteration 6000: 0.001122\n",
      "Cost after iteration 7000: 0.000911\n",
      "Cost after iteration 8000: 0.000762\n",
      "Cost after iteration 9000: 0.000653\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case</th>\n",
       "      <th>781010_HP1118</th>\n",
       "      <th>780792_hslU</th>\n",
       "      <th>781329_rplL</th>\n",
       "      <th>780914_HP0243</th>\n",
       "      <th>781118_HP1293</th>\n",
       "      <th>781673_HP0385</th>\n",
       "      <th>780753_tig</th>\n",
       "      <th>780459_recA</th>\n",
       "      <th>780849_HP0875</th>\n",
       "      <th>781632_HP0371</th>\n",
       "      <th>780394_groES</th>\n",
       "      <th>780913_HP0701</th>\n",
       "      <th>781516_HP1172</th>\n",
       "      <th>781139_tsf</th>\n",
       "      <th>781205_HP0779</th>\n",
       "      <th>781538_fabG</th>\n",
       "      <th>780927_HP1379</th>\n",
       "      <th>780670_acpP</th>\n",
       "      <th>780985_HP0480</th>\n",
       "      <th>781593_HP0073</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.479</td>\n",
       "      <td>2.417</td>\n",
       "      <td>0.913</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.658</td>\n",
       "      <td>0.114</td>\n",
       "      <td>2.274</td>\n",
       "      <td>0.084</td>\n",
       "      <td>1.968</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1.868</td>\n",
       "      <td>0.000</td>\n",
       "      <td>2.465</td>\n",
       "      <td>0.020</td>\n",
       "      <td>2.424</td>\n",
       "      <td>1.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.821</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.015</td>\n",
       "      <td>1.294</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.079</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.632</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>0.918</td>\n",
       "      <td>0.016</td>\n",
       "      <td>1.525</td>\n",
       "      <td>1.457</td>\n",
       "      <td>1.317</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.145</td>\n",
       "      <td>1.332</td>\n",
       "      <td>0.713</td>\n",
       "      <td>1.347</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.187</td>\n",
       "      <td>1.495</td>\n",
       "      <td>0.847</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.404</td>\n",
       "      <td>0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>False</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.014</td>\n",
       "      <td>1.463</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.022</td>\n",
       "      <td>1.451</td>\n",
       "      <td>1.139</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>False</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.520</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>False</td>\n",
       "      <td>1.542</td>\n",
       "      <td>2.207</td>\n",
       "      <td>0.232</td>\n",
       "      <td>2.371</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.017</td>\n",
       "      <td>2.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>True</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.101</td>\n",
       "      <td>1.153</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.118</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.358</td>\n",
       "      <td>0.041</td>\n",
       "      <td>1.935</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>True</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>1.404</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>True</td>\n",
       "      <td>0.048</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.107</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>True</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.205</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.533</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.531</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.630</td>\n",
       "      <td>0.311</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.371</td>\n",
       "      <td>0.017</td>\n",
       "      <td>1.223</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.081</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.425</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.507</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>True</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.196</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.241</td>\n",
       "      <td>1.555</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.412</td>\n",
       "      <td>1.865</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from tabulate import tabulate\n",
    "# Build a model with a n_h-dimensional hidden layer\n",
    "parameters = nn_model(test_X.T, test_Y.T, n_h = 3, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "# Print accuracy\n",
    "predictions = predict(parameters, test_X.T)\n",
    "df_test_X = pd.DataFrame(data=test_X[0:,0:],index=[0,1,2,3,4,5,6,7,8,9,10,11,12,13],columns=columns[1:])\n",
    "predictions_test = pd.DataFrame(data=predictions.T[0:,0:],index=[0,1,2,3,4,5,6,7,8,9,10,11,12,13],columns=[\"Case\"])\n",
    "\n",
    "x = pd.concat([predictions_test,df_test_X],axis =1)\n",
    "\n",
    "display(HTML(x.to_html()))\n",
    "print ('Accuracy: %d' % float((np.dot(test_Y.T,predictions.T) + np.dot(1-test_Y.T,1-predictions.T))/float(test_Y.T.size)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning hidden layer size  ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 1 hidden units: 68.21428571428572 %\n",
      "Accuracy for 2 hidden units: 76.42857142857142 %\n",
      "Accuracy for 3 hidden units: 83.57142857142857 %\n",
      "Accuracy for 4 hidden units: 87.14285714285714 %\n",
      "Accuracy for 15 hidden units: 93.57142857142857 %\n"
     ]
    }
   ],
   "source": [
    "# This may take about 2 minutes to run\n",
    "\n",
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [1, 2, 3, 4, 15, 18]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    parameters = nn_model(train_X.T, train_Y.T, n_h, num_iterations = 10000)\n",
    "    predictions = predict(parameters, train_X.T)\n",
    "    accuracy = float((np.dot(train_Y.T,predictions.T) + np.dot(1-train_Y.T,1-predictions.T))/float(train_Y.T.size)*100)\n",
    "    print (\"Accuracy for {} hidden units: {} %\".format(n_h, accuracy))"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "243px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
